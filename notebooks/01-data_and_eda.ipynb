{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit ('525_group10': conda)",
   "metadata": {
    "interpreter": {
     "hash": "4d65a241df4076a6385b7fab0680e2b0db07f88ff7870c00ab83fc1d89e214d3"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "### Australian Rainfall Data Acquisition and EDA\n",
    "\n",
    "This notebook will be a quick run through of data acquisition and exploratory analysis to evaluate the impact of big data tools. These tools include Dask for out of core processing and Feather/Arrow/Parquet file formats to minimize memory usage and file size. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Dustin\\miniconda3\\envs\\525_group10\\lib\\site-packages\\rpy2\\robjects\\packages.py:366: UserWarning: The symbol 'quartz' is not in this R namespace/package.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import requests\n",
    "import json\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Get helper functions\n",
    "from scripts.utils import combine_australia_rainfall\n",
    "\n",
    "%load_ext rpy2.ipython\n",
    "%load_ext memory_profiler\n",
    "%load_ext autoreload\n",
    "\n",
    "\n",
    "# Constants\n",
    "raw_data_directory_path = os.path.join(\"..\", \"data\", \"raw\")"
   ]
  },
  {
   "source": [
    "First, download the zipped archive folder of all rainfall data - NOTE: THIS CAN TAKE ~ 45 minutes to run:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_progress(block_num, block_size, total_size):\n",
    "    progress_size = int(block_num * block_size)\n",
    "    percent = int(block_num * block_size * 100 / total_size)\n",
    "    sys.stdout.write(\"\\rDownloading... %d%%, %d MB of %d MB\" %\n",
    "                    (percent, progress_size / (1024 * 1024), total_size / (1024 * 1024)))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "def download_from_figshare(article_id, files_to_dl, output_dir):\n",
    "    url = f\"https://api.figshare.com/v2/articles/{article_id}\"\n",
    "    response = requests.request(\"GET\", url, headers={\"Content-Type\": \"application/json\"})\n",
    "    data = json.loads(response.text)\n",
    "    files = data[\"files\"]\n",
    "    for file in files:\n",
    "        if file[\"name\"] in files_to_dl:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            urlretrieve(file[\"download_url\"], os.path.join(output_dir, file[\"name\"]), download_progress)\n",
    "\n",
    "\n",
    "download_from_figshare(\"14096681\", [\"data.zip\"], raw_data_directory_path)"
   ]
  },
  {
   "source": [
    "Next, iterate through the zipped files, extracting the raw csv files:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing File: MPI-ESM-1-2-HAM_daily_rainfall_NSW.csv\n",
      "Processing File: AWI-ESM-1-1-LR_daily_rainfall_NSW.csv\n",
      "Processing File: NorESM2-LM_daily_rainfall_NSW.csv\n",
      "Processing File: ACCESS-CM2_daily_rainfall_NSW.csv\n",
      "Processing File: FGOALS-f3-L_daily_rainfall_NSW.csv\n",
      "Processing File: CMCC-CM2-HR4_daily_rainfall_NSW.csv\n",
      "Processing File: MRI-ESM2-0_daily_rainfall_NSW.csv\n",
      "Processing File: GFDL-CM4_daily_rainfall_NSW.csv\n",
      "Processing File: BCC-CSM2-MR_daily_rainfall_NSW.csv\n",
      "Processing File: EC-Earth3-Veg-LR_daily_rainfall_NSW.csv\n",
      "Processing File: CMCC-ESM2_daily_rainfall_NSW.csv\n",
      "Processing File: NESM3_daily_rainfall_NSW.csv\n",
      "Processing File: MPI-ESM1-2-LR_daily_rainfall_NSW.csv\n",
      "Processing File: ACCESS-ESM1-5_daily_rainfall_NSW.csv\n",
      "Processing File: FGOALS-g3_daily_rainfall_NSW.csv\n",
      "Processing File: INM-CM4-8_daily_rainfall_NSW.csv\n",
      "Processing File: MPI-ESM1-2-HR_daily_rainfall_NSW.csv\n",
      "Processing File: TaiESM1_daily_rainfall_NSW.csv\n",
      "Processing File: NorESM2-MM_daily_rainfall_NSW.csv\n",
      "Processing File: CMCC-CM2-SR5_daily_rainfall_NSW.csv\n",
      "Processing File: observed_daily_rainfall_SYD.csv\n",
      "Processing File: KIOST-ESM_daily_rainfall_NSW.csv\n",
      "Processing File: INM-CM5-0_daily_rainfall_NSW.csv\n",
      "Processing File: MIROC6_daily_rainfall_NSW.csv\n",
      "Processing File: BCC-ESM1_daily_rainfall_NSW.csv\n",
      "Processing File: GFDL-ESM4_daily_rainfall_NSW.csv\n",
      "Processing File: CanESM5_daily_rainfall_NSW.csv\n",
      "Processing File: SAM0-UNICON_daily_rainfall_NSW.csv\n"
     ]
    }
   ],
   "source": [
    "with zipfile.ZipFile(os.path.join(raw_data_directory_path, \"data.zip\"), \"r\") as zf:\n",
    "    file_list = zf.namelist()\n",
    "    for f in file_list:\n",
    "        if \"__MACOSX\" not in f:\n",
    "            print(f\"Processing File: {f}\")\n",
    "            zf.extract(f, path = raw_data_directory_path)\n"
   ]
  },
  {
   "source": [
    "Next we'll evaluate the impact of combining multiple large CSV files using Pandas vs. Dask. We'll check how fast Dask is when creating the task graph as well as materializing to a Pandas dataframe."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Dustin\\miniconda3\\envs\\525_group10\\lib\\site-packages\\numpy\\lib\\arraysetops.py:583: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n",
      "peak memory: 14896.05 MiB, increment: 14751.36 MiB\n",
      "Wall time: 6min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "df_rainfall_pandas = combine_australia_rainfall(base_folder_path=raw_data_directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "df_rainfall_pandas = combine_australia_rainfall(base_folder_path=raw_data_directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "peak memory: 10600.93 MiB, increment: 7.31 MiB\nWall time: 2.86 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "df_rainfall_dask_delay = combine_australia_rainfall(base_folder_path=raw_data_directory_path, method=\"dask\", delay_dask_compute=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "peak memory: 18174.11 MiB, increment: 7580.56 MiB\nWall time: 7min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "combine_australia_rainfall(base_folder_path=raw_data_directory_path, method=\"dask\", delay_dask_compute=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "combine_australia_rainfall(base_folder_path=raw_data_directory_path, method=\"dask\", delay_dask_compute=False)"
   ]
  },
  {
   "source": [
    "Looks like using Dask is ~ 50% faster to read and concatentate multiple large CSV files.\n",
    "\n",
    "Next, we'll look at the speed of determining counts of values by group from the raw dataframe."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "peak memory: 13896.96 MiB, increment: 379.98 MiB\nWall time: 43.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "df_rainfall_pandas[\"model\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "peak memory: 15555.75 MiB, increment: 1717.35 MiB\nWall time: 3min 50s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "model_counts = df_rainfall_dask_delay[\"model\"].value_counts().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pandas_rainfall.head()"
   ]
  },
  {
   "source": [
    "### Challenges Encountered\n",
    "\n",
    "Working with larger data sets on laptop can be difficult due to memory restrictions when loading a dataset. In the exercises in this lab we ran into difficulties with:\n",
    "- Downloading large zip archives:\n",
    "    - Getting the initial data downloaded was difficult as we were downloading one zip archive, and we couldn't parallelize this process. If there had been multiple files we could have used `multiprocessing` to download much quicker.\n",
    "- Combining the dataset / EDA:\n",
    "    - When combining the files with `Pandas` we were loading the entire dataset to memory and this caused issues for some group members with lower RAM. If we knew in advance what calculations we wanted to carry out, we could have delayed execution and used Dask to build the graph and then run an optimized `compute()` step at the end. This can be seen above when combining all files and calculating counts of readings per model using lazy evaluation."
   ],
   "cell_type": "markdown",
   "metadata": {}
  }
 ]
}