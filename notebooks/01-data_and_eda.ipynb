{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Australian Rainfall Data Acquisition and EDA\n",
    "\n",
    "This notebook will be a quick run through of data acquisition and exploratory analysis to evaluate the impact of big data tools. These tools include Dask for out of core processing and Feather/Arrow/Parquet file formats to minimize memory usage and file size.\n",
    "\n",
    "To run this notebook, use the `environment.yml` file in the root of the project to setup a Conda environment. You'll also need R > 4.0.0 installed and need to use:\n",
    "\n",
    "```r\n",
    "install.packages(\"arrow\")\n",
    "```\n",
    "For using advanced file formats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import io\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import gc\n",
    "import requests\n",
    "import json\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "# For different file types\n",
    "import pyarrow.parquet as pq\n",
    "import rpy2.rinterface\n",
    "import rpy2_arrow.pyarrow_rarrow as pyra\n",
    "import pyarrow.feather as feather\n",
    "\n",
    "# Get helper functions\n",
    "from scripts.utils import combine_australia_rainfall\n",
    "\n",
    "%load_ext rpy2.ipython\n",
    "%load_ext memory_profiler\n",
    "%load_ext autoreload\n",
    "\n",
    "\n",
    "# Constants\n",
    "raw_data_directory_path = os.path.join(\"..\", \"data\", \"raw\")\n",
    "processed_data_directory_path = os.path.join(\"..\", \"data\", \"processed\")\n",
    "\n",
    "# Setup folders if not existing\n",
    "os.makedirs(processed_data_directory_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download the zipped archive folder of all rainfall data - NOTE: THIS CAN TAKE ~ 45 minutes to run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading... 100%, 776 MB of 776 MB"
     ]
    }
   ],
   "source": [
    "def download_progress(block_num, block_size, total_size):\n",
    "    progress_size = int(block_num * block_size)\n",
    "    percent = int(block_num * block_size * 100 / total_size)\n",
    "    sys.stdout.write(\"\\rDownloading... %d%%, %d MB of %d MB\" %\n",
    "                    (percent, progress_size / (1024 * 1024), total_size / (1024 * 1024)))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "def download_from_figshare(article_id, files_to_dl, output_dir):\n",
    "    url = f\"https://api.figshare.com/v2/articles/{article_id}\"\n",
    "    response = requests.request(\"GET\", url, headers={\"Content-Type\": \"application/json\"})\n",
    "    data = json.loads(response.text)\n",
    "    files = data[\"files\"]\n",
    "    for file in files:\n",
    "        if file[\"name\"] in files_to_dl:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            urlretrieve(file[\"download_url\"], os.path.join(output_dir, file[\"name\"]), download_progress)\n",
    "\n",
    "\n",
    "download_from_figshare(\"14096681\", [\"data.zip\"], raw_data_directory_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, iterate through the zipped files, extracting the raw csv files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing File: MPI-ESM-1-2-HAM_daily_rainfall_NSW.csv\n",
      "Processing File: AWI-ESM-1-1-LR_daily_rainfall_NSW.csv\n",
      "Processing File: NorESM2-LM_daily_rainfall_NSW.csv\n",
      "Processing File: ACCESS-CM2_daily_rainfall_NSW.csv\n",
      "Processing File: FGOALS-f3-L_daily_rainfall_NSW.csv\n",
      "Processing File: CMCC-CM2-HR4_daily_rainfall_NSW.csv\n",
      "Processing File: MRI-ESM2-0_daily_rainfall_NSW.csv\n",
      "Processing File: GFDL-CM4_daily_rainfall_NSW.csv\n",
      "Processing File: BCC-CSM2-MR_daily_rainfall_NSW.csv\n",
      "Processing File: EC-Earth3-Veg-LR_daily_rainfall_NSW.csv\n",
      "Processing File: CMCC-ESM2_daily_rainfall_NSW.csv\n",
      "Processing File: NESM3_daily_rainfall_NSW.csv\n",
      "Processing File: MPI-ESM1-2-LR_daily_rainfall_NSW.csv\n",
      "Processing File: ACCESS-ESM1-5_daily_rainfall_NSW.csv\n",
      "Processing File: FGOALS-g3_daily_rainfall_NSW.csv\n",
      "Processing File: INM-CM4-8_daily_rainfall_NSW.csv\n",
      "Processing File: MPI-ESM1-2-HR_daily_rainfall_NSW.csv\n",
      "Processing File: TaiESM1_daily_rainfall_NSW.csv\n",
      "Processing File: NorESM2-MM_daily_rainfall_NSW.csv\n",
      "Processing File: CMCC-CM2-SR5_daily_rainfall_NSW.csv\n",
      "Processing File: observed_daily_rainfall_SYD.csv\n",
      "Processing File: KIOST-ESM_daily_rainfall_NSW.csv\n",
      "Processing File: INM-CM5-0_daily_rainfall_NSW.csv\n",
      "Processing File: MIROC6_daily_rainfall_NSW.csv\n",
      "Processing File: BCC-ESM1_daily_rainfall_NSW.csv\n",
      "Processing File: GFDL-ESM4_daily_rainfall_NSW.csv\n",
      "Processing File: CanESM5_daily_rainfall_NSW.csv\n",
      "Processing File: SAM0-UNICON_daily_rainfall_NSW.csv\n"
     ]
    }
   ],
   "source": [
    "with zipfile.ZipFile(os.path.join(raw_data_directory_path, \"data.zip\"), \"r\") as zf:\n",
    "    file_list = zf.namelist()\n",
    "    for f in file_list:\n",
    "        if \"__MACOSX\" not in f:\n",
    "            print(f\"Processing File: {f}\")\n",
    "            zf.extract(f, path = raw_data_directory_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we'll evaluate the impact of combining multiple large CSV files using Pandas vs. Dask. We'll check how fast Dask is when creating the task graph as well as materializing to a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 5417.12 MiB, increment: 5220.13 MiB\n",
      "CPU times: user 8min 58s, sys: 2min 28s, total: 11min 27s\n",
      "Wall time: 15min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "df_rainfall_pandas = combine_australia_rainfall(base_folder_path=raw_data_directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 4459.15 MiB, increment: 543.48 MiB\n",
      "CPU times: user 5min 57s, sys: 3min 42s, total: 9min 40s\n",
      "Wall time: 12min 28s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "combine_australia_rainfall(base_folder_path=raw_data_directory_path, method=\"dask\", delay_dask_compute=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 3453.01 MiB, increment: 3068.04 MiB\n",
      "CPU times: user 11min 46s, sys: 1min 28s, total: 13min 14s\n",
      "Wall time: 16min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "df_rainfall_pandas.to_csv(os.path.join(processed_data_directory_path, \"df_rainfall.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2792.86 MiB, increment: 0.03 MiB\n",
      "CPU times: user 34.9 s, sys: 1min 43s, total: 2min 18s\n",
      "Wall time: 4min 36s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "feather.write_feather(df_rainfall_pandas, os.path.join(processed_data_directory_path, \"df_rainfall.feather\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2364.34 MiB, increment: 1768.98 MiB\n",
      "CPU times: user 4min 4s, sys: 1min 43s, total: 5min 48s\n",
      "Wall time: 9min 40s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "df_rainfall_pandas.to_parquet(os.path.join(processed_data_directory_path, \"df_rainfall.parquet\"),engine='pyarrow', compression='gzip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like using Dask is ~ 30% faster to read and concatentate multiple large CSV files. We also noted that allowing Dask to parallelize and chunk reading CSV files allowed the memory usage to stay lower as well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Large Files\n",
    "\n",
    "We'll read in the combined dataset now looking at different file types/chunk options/columns required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 4231.67 MiB, increment: 3996.91 MiB\n",
      "CPU times: user 42.5 s, sys: 20.9 s, total: 1min 3s\n",
      "Wall time: 48.8 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "df_rainfall_pandas = pd.read_parquet(os.path.join(processed_data_directory_path, \"df_rainfall.parquet\"),engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 3700.44 MiB, increment: 3156.72 MiB\n",
      "CPU times: user 37.7 s, sys: 13.5 s, total: 51.2 s\n",
      "Wall time: 44 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "df_rainfall_pandas = pd.read_parquet(os.path.join(processed_data_directory_path, \"df_rainfall.parquet\"),columns=[\"time\", \"rain (mm/day)\", \"model\"],engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 3214.44 MiB, increment: 2989.17 MiB\n",
      "CPU times: user 21.7 s, sys: 17 s, total: 38.7 s\n",
      "Wall time: 41.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "df_rainfall_pandas = pd.read_feather(os.path.join(processed_data_directory_path, \"df_rainfall.feather\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 3344.96 MiB, increment: 3010.81 MiB\n",
      "CPU times: user 17.8 s, sys: 7.73 s, total: 25.5 s\n",
      "Wall time: 33.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "df_rainfall_pandas = pd.read_feather(os.path.join(processed_data_directory_path, \"df_rainfall.feather\"), columns=[\"time\", \"rain (mm/day)\", \"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2690.53 MiB, increment: 2351.20 MiB\n",
      "CPU times: user 1min 31s, sys: 33.1 s, total: 2min 4s\n",
      "Wall time: 3min 3s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "df_rainfall_pandas = pd.read_csv(os.path.join(processed_data_directory_path, \"df_rainfall.csv\"), index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 3269.80 MiB, increment: 1459.37 MiB\n",
      "CPU times: user 2min 13s, sys: 43.4 s, total: 2min 56s\n",
      "Wall time: 2min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "df_rainfall_pandas = dd.read_csv(os.path.join(processed_data_directory_path, \"df_rainfall.csv\")).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For delayed calculations\n",
    "df_rainfall_dask_delay = dd.read_csv(os.path.join(processed_data_directory_path, \"df_rainfall.csv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA\n",
    "\n",
    "Next, we'll look at the speed of determining counts of values by model from the raw dataframe.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 2561.77 MiB, increment: 292.16 MiB\n",
      "CPU times: user 8.2 s, sys: 504 ms, total: 8.71 s\n",
      "Wall time: 10.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "df_rainfall_pandas[\"model\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 3244.85 MiB, increment: 682.98 MiB\n",
      "CPU times: user 2min 10s, sys: 24.6 s, total: 2min 35s\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "model_counts = df_rainfall_dask_delay[\"model\"].value_counts().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MPI-ESM1-2-HR       5154240\n",
       "TaiESM1             3541230\n",
       "NorESM2-MM          3541230\n",
       "CMCC-CM2-HR4        3541230\n",
       "CMCC-CM2-SR5        3541230\n",
       "CMCC-ESM2           3541230\n",
       "SAM0-UNICON         3541153\n",
       "FGOALS-f3-L         3219300\n",
       "GFDL-CM4            3219300\n",
       "GFDL-ESM4           3219300\n",
       "EC-Earth3-Veg-LR    3037320\n",
       "MRI-ESM2-0          3037320\n",
       "BCC-CSM2-MR         3035340\n",
       "MIROC6              2070900\n",
       "ACCESS-CM2          1932840\n",
       "ACCESS-ESM1-5       1610700\n",
       "INM-CM5-0           1609650\n",
       "INM-CM4-8           1609650\n",
       "KIOST-ESM           1287720\n",
       "FGOALS-g3           1287720\n",
       "MPI-ESM1-2-LR        966420\n",
       "NESM3                966420\n",
       "AWI-ESM-1-1-LR       966420\n",
       "MPI-ESM-1-2-HAM      966420\n",
       "NorESM2-LM           919800\n",
       "BCC-ESM1             551880\n",
       "CanESM5              551880\n",
       "Name: model, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_counts "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(62467843, 7)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rainfall_pandas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>lat_min</th>\n",
       "      <th>lat_max</th>\n",
       "      <th>lon_min</th>\n",
       "      <th>lon_max</th>\n",
       "      <th>rain (mm/day)</th>\n",
       "      <th>model</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1889-01-01 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.244226e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1889-01-02 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.217326e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1889-01-03 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.498125e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1889-01-04 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.251282e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1889-01-05 12:00:00</td>\n",
       "      <td>-35.439867</td>\n",
       "      <td>-33.574619</td>\n",
       "      <td>141.5625</td>\n",
       "      <td>143.4375</td>\n",
       "      <td>4.270161e-13</td>\n",
       "      <td>MPI-ESM-1-2-HAM</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  time    lat_min    lat_max   lon_min   lon_max  \\\n",
       "0  1889-01-01 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "1  1889-01-02 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "2  1889-01-03 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "3  1889-01-04 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "4  1889-01-05 12:00:00 -35.439867 -33.574619  141.5625  143.4375   \n",
       "\n",
       "   rain (mm/day)            model  \n",
       "0   4.244226e-13  MPI-ESM-1-2-HAM  \n",
       "1   4.217326e-13  MPI-ESM-1-2-HAM  \n",
       "2   4.498125e-13  MPI-ESM-1-2-HAM  \n",
       "3   4.251282e-13  MPI-ESM-1-2-HAM  \n",
       "4   4.270161e-13  MPI-ESM-1-2-HAM  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_rainfall_pandas.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that allowing Dask to optimize the task graph speeds up getting counts of values by model fairly significantly. To load the CSV with Pandas, then compute the value counts by group it took ~ (1 min 42 seconds + 15 seconds = 1 min 57 seconds) while for Dask delaying the computation it only took 1 min 19 seconds, an ~ 30% improvement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer between Python and R and EDA in R"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We first begin by loading neccessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "R[write to console]: \n",
      "Attaching package: ‘dplyr’\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:stats’:\n",
      "\n",
      "    filter, lag\n",
      "\n",
      "\n",
      "R[write to console]: The following objects are masked from ‘package:base’:\n",
      "\n",
      "    intersect, setdiff, setequal, union\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%R\n",
    "library(arrow)\n",
    "library(dplyr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Next we attempt to transfer the data using feather and carry out a simple EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.3 s, sys: 24.3 s, total: 50.6 s\n",
      "Wall time: 57 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "r_feather <- arrow::read_feather(\"../data/processed/df_rainfall.feather\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m# A tibble: 27 x 2\u001b[39m\n",
      "\u001b[90m# Groups:   model [27]\u001b[39m\n",
      "   model                  n\n",
      "   \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m              \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m\n",
      "\u001b[90m 1\u001b[39m ACCESS-CM2       1\u001b[4m9\u001b[24m\u001b[4m3\u001b[24m\u001b[4m2\u001b[24m840\n",
      "\u001b[90m 2\u001b[39m ACCESS-ESM1-5    1\u001b[4m6\u001b[24m\u001b[4m1\u001b[24m\u001b[4m0\u001b[24m700\n",
      "\u001b[90m 3\u001b[39m AWI-ESM-1-1-LR    \u001b[4m9\u001b[24m\u001b[4m6\u001b[24m\u001b[4m6\u001b[24m420\n",
      "\u001b[90m 4\u001b[39m BCC-CSM2-MR      3\u001b[4m0\u001b[24m\u001b[4m3\u001b[24m\u001b[4m5\u001b[24m340\n",
      "\u001b[90m 5\u001b[39m BCC-ESM1          \u001b[4m5\u001b[24m\u001b[4m5\u001b[24m\u001b[4m1\u001b[24m880\n",
      "\u001b[90m 6\u001b[39m CMCC-CM2-HR4     3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m 7\u001b[39m CMCC-CM2-SR5     3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m 8\u001b[39m CMCC-ESM2        3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m 9\u001b[39m CanESM5           \u001b[4m5\u001b[24m\u001b[4m5\u001b[24m\u001b[4m1\u001b[24m880\n",
      "\u001b[90m10\u001b[39m EC-Earth3-Veg-LR 3\u001b[4m0\u001b[24m\u001b[4m3\u001b[24m\u001b[4m7\u001b[24m320\n",
      "\u001b[90m# … with 17 more rows\u001b[39m\n",
      "CPU times: user 4.83 s, sys: 3.48 s, total: 8.3 s\n",
      "Wall time: 11.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "\n",
    "r_feather %>%\n",
    "group_by(model) %>%\n",
    "count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> We also attempt to transfer the data using parquet and conducting a simple EDA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 53.2 s, sys: 35.4 s, total: 1min 28s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "r_parquet <- arrow::read_parquet(\"../data/processed/df_rainfall.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[90m# A tibble: 27 x 2\u001b[39m\n",
      "\u001b[90m# Groups:   model [27]\u001b[39m\n",
      "   model                  n\n",
      "   \u001b[3m\u001b[90m<chr>\u001b[39m\u001b[23m              \u001b[3m\u001b[90m<int>\u001b[39m\u001b[23m\n",
      "\u001b[90m 1\u001b[39m ACCESS-CM2       1\u001b[4m9\u001b[24m\u001b[4m3\u001b[24m\u001b[4m2\u001b[24m840\n",
      "\u001b[90m 2\u001b[39m ACCESS-ESM1-5    1\u001b[4m6\u001b[24m\u001b[4m1\u001b[24m\u001b[4m0\u001b[24m700\n",
      "\u001b[90m 3\u001b[39m AWI-ESM-1-1-LR    \u001b[4m9\u001b[24m\u001b[4m6\u001b[24m\u001b[4m6\u001b[24m420\n",
      "\u001b[90m 4\u001b[39m BCC-CSM2-MR      3\u001b[4m0\u001b[24m\u001b[4m3\u001b[24m\u001b[4m5\u001b[24m340\n",
      "\u001b[90m 5\u001b[39m BCC-ESM1          \u001b[4m5\u001b[24m\u001b[4m5\u001b[24m\u001b[4m1\u001b[24m880\n",
      "\u001b[90m 6\u001b[39m CMCC-CM2-HR4     3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m 7\u001b[39m CMCC-CM2-SR5     3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m 8\u001b[39m CMCC-ESM2        3\u001b[4m5\u001b[24m\u001b[4m4\u001b[24m\u001b[4m1\u001b[24m230\n",
      "\u001b[90m 9\u001b[39m CanESM5           \u001b[4m5\u001b[24m\u001b[4m5\u001b[24m\u001b[4m1\u001b[24m880\n",
      "\u001b[90m10\u001b[39m EC-Earth3-Veg-LR 3\u001b[4m0\u001b[24m\u001b[4m3\u001b[24m\u001b[4m7\u001b[24m320\n",
      "\u001b[90m# … with 17 more rows\u001b[39m\n",
      "CPU times: user 4.55 s, sys: 5.73 s, total: 10.3 s\n",
      "Wall time: 17.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%R\n",
    "\n",
    "r_parquet %>%\n",
    "group_by(model) %>%\n",
    "count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final choice\n",
    "\n",
    "> We decided to use feather as the preferred method of making the transfer between Python and R. We made this decision based on our reading of [the material](https://ursalabs.org/blog/2020-feather-v2/) provided in the course notes as well as our empirical results using the `%%time` cell magic. Both parquet and feather are binary file formats, however, feather outperforms parquet on solid state drives due to its simpler compression scheme. Empirically, it was about 65% faster to read the feather file compared to the parquet file. Given the aims and priorities of this milestone, we decided to choose feather as the means for file transfer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenges Encountered\n",
    "\n",
    "Working with larger data sets on laptop can be difficult due to memory restrictions when loading a dataset. In the exercises in this lab we ran into difficulties with:\n",
    "\n",
    "- Downloading large zip archives:\n",
    "    - Getting the initial data downloaded was difficult as we were downloading one zip archive, and we couldn't parallelize this process. If there had been multiple files we could have used `multiprocessing` to download much quicker.\n",
    "- Combining the dataset / EDA:\n",
    "    - When combining the files with `Pandas` we were loading the entire dataset to memory and this caused issues for some group members with lower RAM. \n",
    "    - When saving the dataset, run times and memory usage were highly dependent on file type. Writing the dataset to CSV is memory intensive and takes > 3x as long as writing compressed Parquet files and  ~ 30x as long as writing a Feather file. \n",
    "    - Reading a single large file is much quicker with Feather or parquet file types - even parallelizing reading a CSV with Dask is slower than these more optimized file types.\n",
    "    - If we knew in advance what calculations we wanted to carry out, we could have delayed execution and used Dask to build the graph and then run an optimized `compute()` step at the end. This can be seen above when combining all files and calculating counts of readings per model using lazy evaluation. If our calculation can be done by chunk - we could also read the CSV using Pandas in chunks and calculate our summary statistic. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:525_group10]",
   "language": "python",
   "name": "conda-env-525_group10-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
