{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.2 64-bit ('525_group10': conda)",
   "metadata": {
    "interpreter": {
     "hash": "4d65a241df4076a6385b7fab0680e2b0db07f88ff7870c00ab83fc1d89e214d3"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# Australian Rainfall Data Acquisition and EDA\n",
    "\n",
    "This notebook will be a quick run through of data acquisition and exploratory analysis to evaluate the impact of big data tools. These tools include Dask for out of core processing and Feather/Arrow/Parquet file formats to minimize memory usage and file size.\n",
    "\n",
    "To run this notebook, use the `environment.yml` file in the root of the project to setup a Conda environment. You'll also need R > 4.0.0 installed and need to use:\n",
    "\n",
    "```r\n",
    "install.packages(\"arrow\")\n",
    "```\n",
    "For using advanced file formats."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "C:\\Users\\Dustin\\miniconda3\\envs\\525_group10\\lib\\site-packages\\rpy2\\robjects\\packages.py:366: UserWarning: The symbol 'quartz' is not in this R namespace/package.\n  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import io\n",
    "import os\n",
    "import glob\n",
    "import gc\n",
    "import requests\n",
    "import json\n",
    "from urllib.request import urlretrieve\n",
    "import zipfile\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import dask.dataframe as dd\n",
    "# For different file types\n",
    "import pyarrow.parquet as pq\n",
    "import rpy2.rinterface\n",
    "import rpy2_arrow.pyarrow_rarrow as pyra\n",
    "import pyarrow.feather as feather\n",
    "\n",
    "# Get helper functions\n",
    "from scripts.utils import combine_australia_rainfall\n",
    "\n",
    "%load_ext rpy2.ipython\n",
    "%load_ext memory_profiler\n",
    "%load_ext autoreload\n",
    "\n",
    "\n",
    "# Constants\n",
    "raw_data_directory_path = os.path.join(\"..\", \"data\", \"raw\")\n",
    "processed_data_directory_path = os.path.join(\"..\", \"data\", \"processed\")\n",
    "\n",
    "# Setup folders if not existing\n",
    "os.makedirs(processed_data_directory_path, exist_ok=True)"
   ]
  },
  {
   "source": [
    "First, download the zipped archive folder of all rainfall data - NOTE: THIS CAN TAKE ~ 45 minutes to run:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_progress(block_num, block_size, total_size):\n",
    "    progress_size = int(block_num * block_size)\n",
    "    percent = int(block_num * block_size * 100 / total_size)\n",
    "    sys.stdout.write(\"\\rDownloading... %d%%, %d MB of %d MB\" %\n",
    "                    (percent, progress_size / (1024 * 1024), total_size / (1024 * 1024)))\n",
    "    sys.stdout.flush()\n",
    "\n",
    "\n",
    "def download_from_figshare(article_id, files_to_dl, output_dir):\n",
    "    url = f\"https://api.figshare.com/v2/articles/{article_id}\"\n",
    "    response = requests.request(\"GET\", url, headers={\"Content-Type\": \"application/json\"})\n",
    "    data = json.loads(response.text)\n",
    "    files = data[\"files\"]\n",
    "    for file in files:\n",
    "        if file[\"name\"] in files_to_dl:\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            urlretrieve(file[\"download_url\"], os.path.join(output_dir, file[\"name\"]), download_progress)\n",
    "\n",
    "\n",
    "download_from_figshare(\"14096681\", [\"data.zip\"], raw_data_directory_path)"
   ]
  },
  {
   "source": [
    "Next, iterate through the zipped files, extracting the raw csv files:"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Processing File: MPI-ESM-1-2-HAM_daily_rainfall_NSW.csv\n",
      "Processing File: AWI-ESM-1-1-LR_daily_rainfall_NSW.csv\n",
      "Processing File: NorESM2-LM_daily_rainfall_NSW.csv\n",
      "Processing File: ACCESS-CM2_daily_rainfall_NSW.csv\n",
      "Processing File: FGOALS-f3-L_daily_rainfall_NSW.csv\n",
      "Processing File: CMCC-CM2-HR4_daily_rainfall_NSW.csv\n",
      "Processing File: MRI-ESM2-0_daily_rainfall_NSW.csv\n",
      "Processing File: GFDL-CM4_daily_rainfall_NSW.csv\n",
      "Processing File: BCC-CSM2-MR_daily_rainfall_NSW.csv\n",
      "Processing File: EC-Earth3-Veg-LR_daily_rainfall_NSW.csv\n",
      "Processing File: CMCC-ESM2_daily_rainfall_NSW.csv\n",
      "Processing File: NESM3_daily_rainfall_NSW.csv\n",
      "Processing File: MPI-ESM1-2-LR_daily_rainfall_NSW.csv\n",
      "Processing File: ACCESS-ESM1-5_daily_rainfall_NSW.csv\n",
      "Processing File: FGOALS-g3_daily_rainfall_NSW.csv\n",
      "Processing File: INM-CM4-8_daily_rainfall_NSW.csv\n",
      "Processing File: MPI-ESM1-2-HR_daily_rainfall_NSW.csv\n",
      "Processing File: TaiESM1_daily_rainfall_NSW.csv\n",
      "Processing File: NorESM2-MM_daily_rainfall_NSW.csv\n",
      "Processing File: CMCC-CM2-SR5_daily_rainfall_NSW.csv\n",
      "Processing File: observed_daily_rainfall_SYD.csv\n",
      "Processing File: KIOST-ESM_daily_rainfall_NSW.csv\n",
      "Processing File: INM-CM5-0_daily_rainfall_NSW.csv\n",
      "Processing File: MIROC6_daily_rainfall_NSW.csv\n",
      "Processing File: BCC-ESM1_daily_rainfall_NSW.csv\n",
      "Processing File: GFDL-ESM4_daily_rainfall_NSW.csv\n",
      "Processing File: CanESM5_daily_rainfall_NSW.csv\n",
      "Processing File: SAM0-UNICON_daily_rainfall_NSW.csv\n"
     ]
    }
   ],
   "source": [
    "with zipfile.ZipFile(os.path.join(raw_data_directory_path, \"data.zip\"), \"r\") as zf:\n",
    "    file_list = zf.namelist()\n",
    "    for f in file_list:\n",
    "        if \"__MACOSX\" not in f:\n",
    "            print(f\"Processing File: {f}\")\n",
    "            zf.extract(f, path = raw_data_directory_path)\n"
   ]
  },
  {
   "source": [
    "Next we'll evaluate the impact of combining multiple large CSV files using Pandas vs. Dask. We'll check how fast Dask is when creating the task graph as well as materializing to a Pandas dataframe."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "peak memory: 14989.59 MiB, increment: 14733.35 MiB\nWall time: 12min 24s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "df_rainfall_pandas = combine_australia_rainfall(base_folder_path=raw_data_directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "%%memit\n",
    "combine_australia_rainfall(base_folder_path=raw_data_directory_path, method=\"dask\", delay_dask_compute=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "peak memory: 8657.47 MiB, increment: 4060.77 MiB\nWall time: 12min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "df_rainfall_pandas.to_csv(os.path.join(processed_data_directory_path, \"df_rainfall.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "peak memory: 13136.87 MiB, increment: 2609.02 MiB\nWall time: 39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "feather.write_feather(df_rainfall_pandas, os.path.join(processed_data_directory_path, \"df_rainfall.feather\") )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "peak memory: 11986.00 MiB, increment: 3184.09 MiB\nWall time: 2min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "df_rainfall_pandas.to_parquet(os.path.join(processed_data_directory_path, \"df_rainfall.parquet\"),engine='pyarrow', compression='gzip')"
   ]
  },
  {
   "source": [
    "Looks like using Dask is ~ 30% faster to read and concatentate multiple large CSV files. We also noted that allowing Dask to parallelize and chunk reading CSV files allowed the memory usage to stay lower as well.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Reading Large Files\n",
    "\n",
    "We'll read in the combined dataset now looking at different file types/chunk options/columns required."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "peak memory: 17860.63 MiB, increment: 8927.25 MiB\nWall time: 47.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "df_rainfall_pandas = pd.read_parquet(os.path.join(processed_data_directory_path, \"df_rainfall.parquet\"),engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "peak memory: 14024.64 MiB, increment: 2371.00 MiB\nWall time: 32 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "df_rainfall_pandas = pd.read_parquet(os.path.join(processed_data_directory_path, \"df_rainfall.parquet\"),columns=[\"time\", \"rain (mm/day)\", \"model\"],engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "peak memory: 17895.05 MiB, increment: 7767.96 MiB\nWall time: 23.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "df_rainfall_pandas = pd.read_feather(os.path.join(processed_data_directory_path, \"df_rainfall.feather\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "peak memory: 18995.83 MiB, increment: 1104.16 MiB\nWall time: 22 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "df_rainfall_pandas = pd.read_feather(os.path.join(processed_data_directory_path, \"df_rainfall.feather\"), columns=[\"time\", \"rain (mm/day)\", \"model\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "peak memory: 8678.25 MiB, increment: 8382.47 MiB\nWall time: 1min 42s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "df_rainfall_pandas = pd.read_csv(os.path.join(processed_data_directory_path, \"df_rainfall.csv\"), index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "peak memory: 24421.03 MiB, increment: 5395.96 MiB\nWall time: 1min 57s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "df_rainfall_pandas = dd.read_csv(os.path.join(processed_data_directory_path, \"df_rainfall.csv\")).compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For delayed calculations\n",
    "df_rainfall_dask_delay = dd.read_csv(os.path.join(processed_data_directory_path, \"df_rainfall.csv\"))"
   ]
  },
  {
   "source": [
    "# EDA\n",
    "\n",
    "Next, we'll look at the speed of determining counts of values by model from the raw dataframe.\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "peak memory: 10871.04 MiB, increment: 90.81 MiB\nWall time: 15.6 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "df_rainfall_pandas[\"model\"].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "peak memory: 1873.88 MiB, increment: 1615.46 MiB\nWall time: 1min 19s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "model_counts = df_rainfall_dask_delay[\"model\"].value_counts().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(62467843, 3)"
      ]
     },
     "metadata": {},
     "execution_count": 8
    }
   ],
   "source": [
    "df_rainfall_pandas.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "                  time  rain (mm/day)       model\n",
       "0  1889-01-01 12:00:00   3.293256e-13  ACCESS-CM2\n",
       "1  1889-01-02 12:00:00   0.000000e+00  ACCESS-CM2\n",
       "2  1889-01-03 12:00:00   0.000000e+00  ACCESS-CM2\n",
       "3  1889-01-04 12:00:00   0.000000e+00  ACCESS-CM2\n",
       "4  1889-01-05 12:00:00   1.047658e-02  ACCESS-CM2"
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>time</th>\n      <th>rain (mm/day)</th>\n      <th>model</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1889-01-01 12:00:00</td>\n      <td>3.293256e-13</td>\n      <td>ACCESS-CM2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>1889-01-02 12:00:00</td>\n      <td>0.000000e+00</td>\n      <td>ACCESS-CM2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1889-01-03 12:00:00</td>\n      <td>0.000000e+00</td>\n      <td>ACCESS-CM2</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1889-01-04 12:00:00</td>\n      <td>0.000000e+00</td>\n      <td>ACCESS-CM2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1889-01-05 12:00:00</td>\n      <td>1.047658e-02</td>\n      <td>ACCESS-CM2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 7
    }
   ],
   "source": [
    "df_rainfall_pandas.head()"
   ]
  },
  {
   "source": [
    "We can see that allowing Dask to optimize the task graph speeds up getting counts of values by model fairly significantly. To load the CSV with Pandas, then compute the value counts by group it took ~ (1 min 42 seconds + 15 seconds = 1 min 57 seconds) while for Dask delaying the computation it only took 1 min 19 seconds, an ~ 30% improvement."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "# Challenges Encountered\n",
    "\n",
    "Working with larger data sets on laptop can be difficult due to memory restrictions when loading a dataset. In the exercises in this lab we ran into difficulties with:\n",
    "\n",
    "- Downloading large zip archives:\n",
    "    - Getting the initial data downloaded was difficult as we were downloading one zip archive, and we couldn't parallelize this process. If there had been multiple files we could have used `multiprocessing` to download much quicker.\n",
    "- Combining the dataset / EDA:\n",
    "    - When combining the files with `Pandas` we were loading the entire dataset to memory and this caused issues for some group members with lower RAM. \n",
    "    - When saving the dataset, run times and memory usage were highly dependent on file type. Writing the dataset to CSV is memory intensive and takes > 3x as long as writing compressed Parquet files and  ~ 30x as long as writing a Feather file. \n",
    "    - Reading a single large file is much quicker with Feather or parquet file types - even parallelizing reading a CSV with Dask is slower than these more optimized file types.\n",
    "    - If we knew in advance what calculations we wanted to carry out, we could have delayed execution and used Dask to build the graph and then run an optimized `compute()` step at the end. This can be seen above when combining all files and calculating counts of readings per model using lazy evaluation. If our calculation can be done by chunk - we could also read the CSV using Pandas in chunks and calculate our summary statistic. "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}